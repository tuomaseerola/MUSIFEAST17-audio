[
  {
    "objectID": "MUSIFEST17_audio_features.html",
    "href": "MUSIFEST17_audio_features.html",
    "title": "MUSIFEAST17 Audio Features",
    "section": "",
    "text": "Audio features for MUSIFEAST17 dataset extracted with MIR Toolbox.\nMUSIFEAST-17 (MUsic Stimuli for Imagination, Familiarity, Emotion, and Aesthetic STudies across 17 genres) is a dataset created by Hazel Aileen van der Walle, Wei Wu, Elizabeth Margulis, and Kelly Jakubowski. It is available at OSF at https://osf.io/5ebz2/. A paper describing the creation of the dataset is under review (van der Walle, H. A., Wu, W., Margulis, E. H., & Jakubowski, K. (under review). MUSIFEAST-17: MUsic Stimuli for Imagination, Familiarity, Emotion, and Aesthetic STudies across 17 genres.).\nThis section offers two pre-calculated features for the dataset, (1) a full feature set and (2) trimmed 33-feature set. Both of these features have been extracted with the MIR Toolbox (Lartillot et al., 2007), which is a high-level toolbox for extracting low-level features from audio files. The features are extracted from the audio files using a custom set of commands, where all analyses are carried out in windowed analyses (analysis window length vary across the features). The features are stored in a tab-separated text file with the following columns labelled as filename and the feature names."
  },
  {
    "objectID": "MUSIFEST17_audio_features.html#full-feature-set",
    "href": "MUSIFEST17_audio_features.html#full-feature-set",
    "title": "MUSIFEAST17 Audio Features",
    "section": "1 Full feature set",
    "text": "1 Full feature set\n# From custom matlab function\ndf &lt;- read.csv('audio_features/MUSIFEAST17_all_features.csv',header=TRUE,sep='\\t')\ncolnames(df)[1]&lt;-'filename'\nknitr::kable(head(df[,1:4]),digits=2)\n\n\n\n\n\n\n\n\n\nfilename\ndynamics_rms_Mean\ndynamics_rms_Std\ndynamics_rms_Slope\n\n\n\n\nAmbient_HIGH_08.mp3\n0.11\n0.03\n0.28\n\n\nAmbient_HIGH_09.mp3\n0.06\n0.02\n1.93\n\n\nAmbient_HIGH_10.mp3\n0.07\n0.04\n0.62\n\n\nAmbient_HIGH_11.mp3\n0.12\n0.03\n1.78\n\n\nAmbient_HIGH_12.mp3\n0.08\n0.03\n1.41\n\n\nAmbient_HIGH_13.mp3\n0.03\n0.03\n2.09\n\n\n\nprint(paste(nrow(df),'Observations x',ncol(df),'Features'))\n[1] “356 Observations x 242 Features”\nFor the full feature set, we have 241 features, which is a lot considering we have 356 music examples."
  },
  {
    "objectID": "MUSIFEST17_audio_features.html#reduced-33-feature-set",
    "href": "MUSIFEST17_audio_features.html#reduced-33-feature-set",
    "title": "MUSIFEAST17 Audio Features",
    "section": "2 Reduced, 33-feature set",
    "text": "2 Reduced, 33-feature set\nA trimmed set of features originates from a past paper (Eerola, 2011) and subsequent studies that resorted to a small number of audio-based features (e.g., Laukka et al., 2013). The idea here is to have a set of features that are not too many, but still cover the most important aspects of music. The original set was based on a set of 37 or 39 features, but here we have reduced these into 33 features. These are the features defined and used in past studies (M refers to mean, SD refers to standard deviation, SL refers to slope, and C refers to centroid).\nThe features are divided into seven categories, which are listed below:\n\nDynamics\n\n1–3 RMS energy (M, SD, SL)\n4 Low-Energy ratio (M)\n\n\n\nArticulation\n\n5–6 Attack Time (M, SD)\n7 Attack Slope (M)\n8 Event Density (M)\n\n\n\nRhythm\n\n9 Fluctuation Peak (M)\n10–11 Fluctuation Peak (mag.) (M, C)\n12–13 Tempo (M, SD)\n14–15 Pulse Clarity (M, SD)\n\n\n\nTimbre\n\n16–17 Spectral Centroid (M, SD)\n\n18 Spectral Spread (M)\n19–20 Roughness (M, SD)\n21–22 Spectral Flux (M, SD)\n23–24 Regularity (M, SD)\n25–26 Inharmonicity (M, SD)\n\n\n\nPitch\n\n27–28 Pitch (M, SD)\n29–30 Chromagram (unwrapped centr.) (M, SD)\n\n\n\nTonal\n\n31–32 Key Clarity (M, SD)\n33 Key Mode (majorness) (M, SD)\n34 HCDF (M)\n35 Spectral Entropy (oct. coll.) (M)\n\n\n\nStructure\n\n36 Repetition (Spectrum) (M)\n37 Repetition (Rhythm) (M)\n38 Repetition (Tonal) (M)\n39 Repetition (Register) (M)\n\nHowever, things have moved on from 2011 and now we have 33 features. From the original features, we have reduced 2 features as obsolete: (Inharmonicity M and SD) cannot be calculated from polyphonic audio. We have also reduced some other features because of high collinearity and frequent missing values in the analyses. Note that only some of these features have received empirical validation in the past (see Lange & Frieler, 2018; Panda, Malheiro, & Paiva, 2023)."
  },
  {
    "objectID": "MUSIFEST17_audio_features.html#feature-distributions",
    "href": "MUSIFEST17_audio_features.html#feature-distributions",
    "title": "MUSIFEAST17 Audio Features",
    "section": "3 Feature distributions",
    "text": "3 Feature distributions\nThe 33 feature set are actually just a list of features names used to trim the full feature set down to 33 features. Here we read the list and filter the full feature set with these names, rename three features for consistency, and plot the distributions of the features.\n\nfeature_set &lt;- read.csv('audio_features/feature_set33_matlab.txt',header=FALSE); feature_set&lt;-feature_set$V1\nkeep_vars &lt;- c('filename')\ndf2 &lt;- dplyr::select(df,all_of(c(keep_vars,feature_set)))\n\n# rename categories of three features\nnames(df2)[which(names(df2)=='rhythm_attack_time_Mean')] &lt;- 'articulation_attack_time_Mean'\nnames(df2)[which(names(df2)=='rhythm_attack_time_Std')] &lt;- 'articulation_attack_time_Std'\nnames(df2)[which(names(df2)=='rhythm_attack_time_Slope')] &lt;- 'articulation_attack_time_Slope'\n\ntmp &lt;- pivot_longer(df2,cols=2:34,names_to='feature',values_to='value')\ntmp$feature&lt;-as.factor(tmp$feature)\nggplot(tmp,aes(x=value)) + geom_histogram(bins = 24,fill='lightblue1',color='black') + facet_wrap(~feature,scales='free') + theme_bw()\n\n\n\n\n\n\n\n\n\n3.1 Visualise features categories\nThe features are divided into seven categories, and we have 4-6 features per category. Let’s explore the similarities of the feature categories visually by plotting a heatmap of the correlation matrix of the features. The features are grouped by category, and the correlation matrix is calculated for each feature within the category. In some cases, the features within the categories are clearly distinct from the other categories similar (high correlation) within a category (structure, pitch, and articulation), and some have multiple features but not all that are similar within the category (timbre, tonal, rhythm, dynamics). A more formal would explore the classification accuracy of the feature categories with the feature values that would require a larger dataset, here we just illustrate the way the features are attempting to capture various aspects of music.\n\n\n\nCategory\nFreq\n\n\n\n\narticulation\n4\n\n\ndynamics\n4\n\n\npitch\n4\n\n\nrhythm\n5\n\n\nstructure\n4\n\n\ntimbre\n6\n\n\ntonal\n6"
  },
  {
    "objectID": "MUSIFEST17_audio_features.html#feature-and-ratings",
    "href": "MUSIFEST17_audio_features.html#feature-and-ratings",
    "title": "MUSIFEAST17 Audio Features",
    "section": "4 Feature and Ratings",
    "text": "4 Feature and Ratings\nExplore whether features provide any fit for the data. The behavioural ratings can be obtained from OSF, https://osf.io/5ebz2/.\nr &lt;- read.csv('behavioural_data/Raw_data_cleaned_final.csv',header=TRUE)\nr &lt;- dplyr::filter(r,renamed != '60s_LOW_08') # remove eliminated clip that had lyrics\n\nS &lt;- summarise(group_by(r,renamed,genre),\n               valence=mean(clip_response.Valence,na.rm=TRUE),\n               arousal=mean(clip_response.Arousal,na.rm=TRUE),.groups='drop')\n# Make filenames comparable\ndf2$filename_short &lt;- str_replace_all(df2$filename,'.mp3$','') \ndf3 &lt;- merge(df2,S,by.x='filename_short',by.y='renamed')\nprint(paste(nrow(df3),'Observations x',ncol(df3),'Columns'))\n[1] “356 Observations x 38 Columns”\n\n4.1 Predict ratings with features\nJust a casual analysis to get a feel for features and emotions, which can typically be mapped together fairly well. This is not a carefully constructed analysis nor contains cross-validation.\nLet’s do this with 6 PCA components from 33 features.\npca &lt;- prcomp(df3[,3:35], scale = TRUE, center = TRUE) # do the PCA on training set\nN &lt;- 6\nm1 &lt;- lm(valence~., data = data.frame(pca$x[,1:N],valence=df3$valence))\ns1 &lt;- summary(m1)\ncat(paste0('$R^{2adj}_{Valence}$=',format(round(s1$adj.r.squared,2),nsmall=2)))\n\\(R^{2adj}_{Valence}\\)=0.22\nm2 &lt;- lm(arousal~., data = data.frame(pca$x[,1:N],arousal=df3$arousal))\ns2 &lt;- summary(m2)\ncat(paste0('$R^{2adj}_{Arousal}$=',format(round(s2$adj.r.squared,2),nsmall=2)))\n\\(R^{2adj}_{Arousal}\\)=0.58"
  },
  {
    "objectID": "MUSIFEST17_audio_features.html#references",
    "href": "MUSIFEST17_audio_features.html#references",
    "title": "MUSIFEAST17 Audio Features",
    "section": "5 References",
    "text": "5 References\n\nEerola, T. (2011). Are the emotions expressed in music genre-specific? An audio-based evaluation of datasets spanning classical, film, pop and mixed genres. Journal of New Music Research, 40(4), 349-366. https://doi.org/10.1080/09298215.2011.602195\nLange, E. B. & Frieler, K. (2018). Challenges and opportunities of predicting musical emotions with perceptual and automatized features. Music Perception: An Interdisciplinary Journal, 36(2), 217–242.\nLartillot, O., Toiviainen, P., & Eerola, T. (2008). A Matlab Toolbox for Music Information Retrieval. In C. Preisach, H. Burkhardt, L. Schmidt-Thieme, & R. Decker (Eds.), Data Analysis, Machine Learning and Applications. Studies in Classification, Data Analysis, and Knowledge Organization (pp. 261-268). Berlin, Germany: Springer.\nLaukka, P., Eerola, T., Thingujam, N. S., Yamasaki, T., & Beller, G. (2013). Universal and Culture-Specific Factors in the Recognition and Performance of Musical Emotions. Emotion, 13(3), 434-449. https://doi.org/10.1037/a0031388\nPanda, R., Malheiro, R., & Paiva, R. P. (2023). Audio Features for Music Emotion Recognition: A Survey. IEEE Transactions on Affective Computing, 14(1), 68-88."
  }
]